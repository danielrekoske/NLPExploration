{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Bigram Model\n",
    "\n",
    "### A simple bigram model will help motivate more intricate transformer architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Bigram?\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2.\n",
    "\n",
    "The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "\n",
    "A bigram model then involves predicting the following token when given a singular preceeding token. \n",
    "\n",
    "Let's spell this out in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import PyTorch in python which is a machine learning framework based on the Torch library and the Python language. It is primarily used for creating deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Now we must define some hyperparameters for our Bigram Model. \n",
    "\n",
    "***These parameters are taken from Andrej Karpathy's Let's Build GPT YouTube video***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 8 \n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "Batch size is the number of samples used in one forward and backward pass through the network. In principle, batch size determines the number of independent sequences we process in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Size\n",
    "Block size is snynonymous with context length or the context window. It determines the number of tokens considered when predicting a new token.\n",
    "\n",
    "With our block size of 8, a maximum of 8 tokens will be used to predict the 9th token in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding the torch.Generator Object\n",
    "\n",
    "We will be generating random numbers here. To ensure the random numbers are the same everytime we run the preceeding and following code sequence, we must create a torch generator object with a manual seed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10aa83a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_len = len(chars)\n",
    "# encode chars as integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "#anon functions to encode from char to integer and decode from integer to char\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate batch of inouts and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ints = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ints])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ints])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Estimate the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Bigram Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Instance of Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_len)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Estimated Loss with Adam optimizer\n",
    "\n",
    "Documentation for adam optimizer here: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "\n",
    "Adam focuses on two ideas\n",
    "\n",
    "Mometum and RMSprop\n",
    "\n",
    "\n",
    "Momentum: Helps the optimizer to keep moving in the current direction, similar to the physical concept of momentum. It introduces a moving average of the gradients, and this moving average is then used to update the parameters of the model.\n",
    "\n",
    "Here's a basic idea of how momentum works:\n",
    "\n",
    "1. **Update Rule:** Instead of updating the parameters based solely on the current gradient, momentum introduces a moving average of the past gradients. The update rule for a parameter \\(w\\) becomes:\n",
    "\n",
    "   $ v_t = \\beta \\cdot v_{t-1} + (1 - \\beta) \\cdot \\nabla J(w_t) $ \\\n",
    "   $ w_{t+1} = w_t - \\alpha \\cdot v_t $\n",
    "\n",
    "   Where:\n",
    "   - $ \\alpha $ is the learning rate.\n",
    "   - $ \\beta$ is the momentum term (typically close to 1, e.g., 0.9 or 0.99).\n",
    "   - $ \\nabla J(w_t)$ is the gradient of the loss with respect to the parameters at the current iteration. \n",
    "\n",
    "2. **Benefits:** Momentum helps to smooth out oscillations and speed up convergence, especially in the presence of noisy gradients or if the optimization surface has long, shallow valleys. It helps the optimizer to continue moving in the current direction, even if the gradient changes direction frequently.\n",
    "\n",
    "3. **Intuition:** Think of it as a ball rolling down a surface with valleys and hills. Momentum allows the ball to accumulate speed when rolling down a slope and carry that speed to overcome small bumps, helping it to converge faster.\n",
    "\n",
    "RMSprop:\n",
    "\n",
    "1. **Adaptive Learning Rates:** Similar to Adam, RMSprop adapts the learning rates for each parameter individually. It does so by dividing the learning rate for a parameter by the square root of the exponential moving average of the squared gradients.\n",
    "\n",
    "   $ v_t = \\beta \\cdot v_{t-1} + (1 - \\beta) \\cdot (\\nabla J(w_t))^2 $\\\n",
    "   $ w_{t+1} = w_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\cdot \\nabla J(w_t) $\n",
    "\n",
    "   Where:\n",
    "   - $ \\alpha $ is the learning rate.\n",
    "   - $ \\beta $ is a decay term (typically close to 1, e.g., 0.9).\n",
    "   - $ \\epsilon $ is a small constant added for numerical stability.\n",
    "   - $ \\nabla J(w_t) $ is the gradient of the loss with respect to the parameters at the current iteration. \n",
    "\n",
    "2. **Benefits:** RMSprop helps address the problem of vanishing or exploding gradients by normalizing the updates. It is particularly effective in scenarios where the scale of the gradients varies widely across different parameters or time steps.\n",
    "\n",
    "3. **Exponential Moving Average:** The use of the exponential moving average for squared gradients helps RMSprop to adapt its learning rates dynamically. It focuses more on recent information and less on historical gradients.\n",
    "\n",
    "4. **Intuition:** RMSprop can be thought of as adjusting the learning rates based on the historical magnitudes of the gradients. It scales down the learning rates for parameters with large and frequent updates, allowing for more stable and efficient convergence.\n",
    "\n",
    "RMSprop is a key component in the family of adaptive learning rate optimization algorithms and is widely used in practice for training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielrekoske/NLPTransformer/Transformer.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create a PyTorch optimizer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate From the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielrekoske/NLPTransformer/Transformer.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# generate from the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielrekoske/NLPTransformer/Transformer.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(decode(m\u001b[39m.\u001b[39mgenerate(context, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT - Generative Pretrained Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Transformer Architecture\n",
    "\n",
    "## Motivation/Intuition\n",
    "\n",
    "The Transformer architecture, introduced in the paper \"Attention is All You Need,\" revolutionized sequence-to-sequence tasks in machine learning. Its key motivation is to capture long-range dependencies in sequences more efficiently, enabling parallelization.\n",
    "\n",
    "## Components of the Transformer\n",
    "![Transformer Architecture](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)\n",
    "\n",
    "### 1. Self-Attention Mechanism\n",
    "![Self-Attention Mechanism](https://theaisummer.com/static/e497f0d469418119f9db9c53b9851e61/b9460/self-attention-explained.png)\n",
    "#### Overview\n",
    "\n",
    "Self-attention is a mechanism that allows a model to focus on different parts of input sequences with varying degrees of emphasis. It is particularly popular in natural language processing tasks, such as machine translation and text summarization. The self-attention mechanism enables a model to weigh the importance of different words in a sequence when making predictions, capturing long-range dependencies effectively.\n",
    "\n",
    "#### Input Representation\n",
    "\n",
    "Suppose we have an input sequence $ X = \\{x_1, x_2, ..., x_n\\} $, where $n$ is the sequence length. Each $x_i$ represents the embedding of the $i$-th element in the sequence.\n",
    "\n",
    "#### Key, Query, and Value Representations\n",
    "\n",
    "For self-attention, we create three linear projections for each element in the sequence:\n",
    "\n",
    "- **Key ($K$):** $K = X \\cdot W_K$, where $W_K$ is a learnable weight matrix.\n",
    "- **Query ($Q$):** $Q = X \\cdot W_Q$, where $W_Q$ is a learnable weight matrix.\n",
    "- **Value ($V$):** $V = X \\cdot W_V$, where $W_V$ is a learnable weight matrix.\n",
    "\n",
    "#### Attention Scores\n",
    "\n",
    "The attention scores are computed by taking the dot product of the query and key representations and then scaling by the square root of the dimension of the key vectors:\n",
    "\n",
    "$ \\text{Attention Scores} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}$\n",
    "\n",
    "Here, $d_k$ is the dimension of the key vectors. The scaling helps prevent the dot products from becoming too large, which can lead to overly small gradients in the training process.\n",
    "\n",
    "#### Softmax and Weighted Sum\n",
    "\n",
    "$ \\text{Softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $\n",
    "\n",
    "Here, $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$ is a vector of real numbers, and the softmax function computes the probability distribution over these values. The numerator $e^{x_i}$ represents the exponential of the $i$-th element of the input vector, and the denominator $\\sum_{j=1}^{n} e^{x_j}$ is the sum of exponentials over all elements in the vector. The result is a probability distribution where each element is in the range (0, 1), and the sum of all elements is equal to 1.\n",
    "\n",
    "Apply the softmax function to the attention scores to obtain normalized weights:\n",
    "\n",
    "$ \\text{Attention Weights} = \\text{softmax}(\\text{Attention Scores}) $\n",
    "\n",
    "\n",
    "Finally, compute the weighted sum of the value vectors using the attention weights:\n",
    "\n",
    "$\\text{Output} = \\text{Attention Weights} \\cdot V$\n",
    "\n",
    "### 2. Multi-Head Attention\n",
    "\n",
    "![Multi-Head Attention](https://production-media.paperswithcode.com/methods/3080f470-be1b-48d6-b1e8-43edb0a71739.png)\n",
    "\n",
    "#### Single-Head Attention\n",
    "\n",
    "For a single head, given a set of input vectors $X = \\{x_1, x_2, ..., x_n\\}$, and the corresponding query (Q), key (K), and value (V) matrices $W_Q$, $W_K$, and $W_V$, the attention scores for the $i$-th element are computed as follows:\n",
    "\n",
    "$ \\text{Attention}(Q, K, V)_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i $\n",
    "\n",
    "where $Q_i$, $K_i$, and $V_i$ represent the $i$-th rows of the matrices $Q$, $K$, and $V$, and $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "For multi-head attention with $H$ heads, let $Q_i^h$, $K_i^h$, and $V_i^h$ denote the query, key, and value vectors for the $i$-th element in the $h$-th head, respectively.\n",
    "\n",
    "The output $O_i$ for the $i$-th element in the $h$-th head is given by:\n",
    "\n",
    "$ O_i^h = \\text{Attention}(Q^hW_{Qi}, K^hW_{Ki}, V^hW_{Vi}) $\n",
    "\n",
    "where $W_{Qi}$, $W_{Ki}$, and $W_{Vi}$ are the weight matrices for the $h$-th head.\n",
    "\n",
    "The final output $O_i$ is obtained by concatenating the outputs from all heads and applying a linear transformation:\n",
    "\n",
    "$ O_i = \\text{Concat}(O_i^1, O_i^2, ..., O_i^H)W_O $\n",
    "\n",
    "where $W_O$ is the weight matrix for the final linear transformation.\n",
    "\n",
    "\n",
    "### 3. Positional Encoding\n",
    "\n",
    "![Positional Encoding](https://machinelearningmastery.com/wp-content/uploads/2022/01/PE1.png)\n",
    "\n",
    "Positional Encoding Formulas:\n",
    "1. $ \\text{PE}(pos, 2i) = \\sin\\left(\\frac{{pos}}{{10000^{(2i/d)}}}\\right) $\n",
    "2. $ \\text{PE}(pos, 2i + 1) = \\cos\\left(\\frac{{pos}}{{10000^{(2i/d)}}}\\right) $\n",
    "\n",
    "Where:\n",
    "- $ pos $ is the position of the token in the sequence.\n",
    "- $ i $ is the dimension index of the positional encoding.\n",
    "- $ d $ is the dimensionality of the positional encoding, typically the same as the embedding dimension.\n",
    "\n",
    "The positional encoding matrix is then added element-wise to the input embeddings to incorporate positional information in the GPT architecture.\n",
    "\n",
    "\n",
    "### 4. Encoder and Decoder Stacks\n",
    "\n",
    "The Transformer architecture comprises stacks of encoders and decoders. The encoder processes the input sequence, while the decoder generates the output sequence. Each encoder and decoder consist of multiple layers, each containing self-attention and feedforward sub-layers.\n",
    "\n",
    "![Encoder and Decoder Stacks](https://www.researchgate.net/publication/360353665/figure/fig4/AS:1151883645329415@1651641870298/Stacked-encoder-and-decoder-blocks-used-in-Transformers-are-presented-for-the-machine.ppm)\n",
    "\n",
    "### 5. Feedforward Networks\n",
    "\n",
    "After the self-attention mechanism, each sub-layer in the encoder and decoder stacks contains a feedforward neural network. This network is responsible for transforming the high-dimensional output of the attention mechanism into a more suitable form for the next layer.\n",
    "\n",
    "![Feedforward Networks](https://learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_len, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_len)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.810461 M parameters\n",
      "step 0: train loss 4.5722, val loss 4.5314\n",
      "step 500: train loss 1.4393, val loss 2.6423\n",
      "step 1000: train loss 1.1005, val loss 2.5024\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the loss on train and val sets on varying iterations\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate from the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(m\u001b[38;5;241m.\u001b[39mgenerate(context, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
